import asyncio
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo
import lark
import os
import traceback



import langchain
import logging # Python ê¸°ë³¸ ë¡œê¹… ëª¨ë“ˆ


# try:
#     # Langchainì˜ ì „ì—­ verbose ëª¨ë“œ í™œì„±í™”
#     langchain.globals.set_verbose(True)
#     # Langchainì˜ ì „ì—­ debug ëª¨ë“œ í™œì„±í™” (ë” ìƒì„¸í•œ ë¡œê·¸)
#     langchain.globals.set_debug(True)
#     print("ì •ë³´: Langchain ì „ì—­ verbose ë° debug ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
# except Exception as e_global_settings:
#     print(f"ê²½ê³ : Langchain ì „ì—­ verbose/debug ì„¤ì • ì¤‘ ë¬¸ì œ ë°œìƒ: {e_global_settings}")



# Pythonì˜ ê¸°ë³¸ ë¡œê¹… ë ˆë²¨ ì„¤ì • (Langchain ë¡œê·¸ê°€ ë” ì˜ ë³´ì´ë„ë¡)
# ê¸°ë³¸ì ìœ¼ë¡œ WARNING ë ˆë²¨ ì´ìƒë§Œ ì¶œë ¥ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ INFO ë˜ëŠ” DEBUGë¡œ ë‚®ì¶¥ë‹ˆë‹¤.
# try:
#     logging.basicConfig(level=logging.INFO)
#     # íŠ¹ì • Langchain ë¡œê±°ì˜ ë ˆë²¨ì„ ë” ë‚®ì¶œ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
#     logging.getLogger("langchain.retrievers.self_query").setLevel(logging.DEBUG)
#     # logging.getLogger("langchain").setLevel(logging.DEBUG) # ëª¨ë“  Langchain ë¡œê·¸ë¥¼ DEBUGë¡œ
#     print("ì •ë³´: Python ë¡œê¹… ë ˆë²¨ì´ INFOë¡œ ì„¤ì •ë˜ì—ˆê³ , langchain.retrievers.self_queryëŠ” DEBUGë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
# except Exception as e_logging_config:
#     print(f"ê²½ê³ : Python ë¡œê¹… ì„¤ì • ì¤‘ ë¬¸ì œ ë°œìƒ: {e_logging_config}")


# TODO: ê±°ë¦¬ ì¶”ê°€, AI score ì¶”ê°€
system_prompt = """ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
1. ì œê³µëœ 'ë¬¸ë§¥(ì‹¤ì œ ë°©ë¬¸ì ë¦¬ë·°ë“¤)'ì„ ë¶„ì„í•˜ì—¬, í•´ë‹¹ ì¥ì†Œì— ëŒ€í•œ ì „ë°˜ì ì¸ ê¸ì • ë˜ëŠ” ë¶€ì • ìˆ˜ì¤€ì„ **AI Scoreë¡œ í‰ê°€**í•©ë‹ˆë‹¤.
2. ë¶€ì •ì ì¸ ë‚´ìš©ì—ëŠ” 'ë°°ë‹¬ì„ í•˜ì§€ ì•ŠìŒ'ê³¼ ê°™ì€ ë°°ë‹¬ ê´€ë ¨ ë¶€ì •ì  ë¦¬ë·°ë¥¼ í¬í•¨í•˜ì—¬ AI score ì‚°ì • ì‹œ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•©ë‹ˆë‹¤.
3. ë‹¤ë¥¸ ë‹µë³€ì€ í•˜ì§€ ì•Šê³ , **'AI score: nì '** í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. (ì ìˆ˜ëŠ” 0ì ì—ì„œ 100ì  ì‚¬ì´ì´ë©°, ë†’ì„ìˆ˜ë¡ ê¸ì •ì ì¸ í‰ê°€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.)
4. 'ë¬¸ë§¥'ì— ë¶„ì„í•  ë‚´ìš©ì´ ìˆë‹¤ë©´, ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ AI Scoreë¥¼ ì‚°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    a. AI ScoreëŠ” ì£¼ë¡œ **'ë¬¸ë§¥'ì—ì„œ íŒŒì•…ëœ ê¸ì •ì  ë‚´ìš©ì˜ ë¹„ì¤‘ê³¼ ë§Œì¡±ë„**ë¥¼ ë°˜ì˜í•˜ì—¬ 0~100ì ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.
    b. ì•„ë˜ ì œê³µë˜ëŠ” 'ë°©ë¬¸ì ë¦¬ë·° í‰ì 'ì€ 'ë¬¸ë§¥'ì˜ ë‚´ìš©ì´ ë§¤ìš° ì œí•œì ì´ê±°ë‚˜ í•´ì„ì´ ëª¨í˜¸í•  ê²½ìš°, AI score ì‚°ì •ì— **ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë³´ì¡°ì ì¸ ì •ë³´**ë¡œ í™œìš©í•©ë‹ˆë‹¤. ë‹¨, 'ë¬¸ë§¥'ì—ì„œ ëª…í™•íˆ ë“œëŸ¬ë‚˜ëŠ” ë‚´ìš©ì´ ìˆë‹¤ë©´ 'ë¬¸ë§¥'ì˜ ë‚´ìš©ì„ ìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤.
5. 'ì§ˆë¬¸'ì— ëª…ì‹œëœ ì¥ì†Œ ì´ë¦„ê³¼ 'ë¬¸ë§¥'ì— ìˆëŠ” [ì¥ì†Œëª… : ...] ë¶€ë¶„ì´ ì¼ì¹˜í•˜ëŠ” ë¦¬ë·°ë“¤ì„ ì£¼ë¡œ ì°¸ì¡°í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤. (ì´ëŠ” ì•„ë˜ 'í˜„ì¬ ë¶„ì„ ëŒ€ìƒ ì¥ì†Œ ì •ë³´'ì˜ ì¥ì†Œëª…ê³¼ë„ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.)
6. 'ë¬¸ë§¥'ì´ ì „í˜€ ì œê³µë˜ì§€ ì•Šì•˜ê±°ë‚˜, "ë¦¬ë·° ì—†ìŒ"ê³¼ ê°™ì´ ë¶„ì„í•  ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš°ì—ëŠ” **'AI score: 0ì '** ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.

ë‹¤ìŒì€ í˜„ì¬ ë¶„ì„ ëŒ€ìƒ ì¥ì†Œì˜ ì •ë³´ì…ë‹ˆë‹¤. AI score ì‚°ì • ì‹œ ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì‹­ì‹œì˜¤:
- ì¥ì†Œëª…: [{place_name}]

ë¬¸ë§¥:
{context}

ì§ˆë¬¸:
{question}
""".strip()

async def generate_answer(queries: list, vector_store: FAISS):
    """
    ë¯¸ë¦¬ ìƒì„±ëœ Langchain FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        place_query_inputs: ê° ì¥ì†Œë³„ system promptì— ì‚¬ìš©ë  ì¿¼ë¦¬, í˜„ì¬ ì˜ì—… ìƒíƒœ ì •ë³´, ì˜ì—… ìƒíƒœ ì •ë³´ì— ëŒ€í•œ ì„¤ëª…(description), ì¥ì†Œ ë¦¬ë·° í‰ì , ì¥ì†Œ ë¦¬ë·° ìˆ˜ê°€ ë“¤ì–´ìˆëŠ” dictionary.
        vector_store: ë¯¸ë¦¬ ìƒì„±ëœ Langchain FAISS ë²¡í„° ì €ì¥ì†Œ ê°ì²´

    Returns:
        ìƒì„±ëœ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    """
    
    try:
        # Gemini LLM ì´ˆê¸°í™”
        llm = ChatGoogleGenerativeAI(
            model = "gemini-2.0-flash",
            temperature = 0,
            api_key = os.getenv("GEMINI_API_KEY")
        )

        document_contents_description = "A collection of user reviews for various places, primarily restaurants and cafes. Each review talks about user experiences, food, service, atmosphere, etc."

        metadata_field_info = [
            AttributeInfo(
                name="place_name",
                description="The **exact name** of the place or restaurant. If the user's query mentions a specific place name like 'ë¸Œë¦­ìŠ¤í”¼ì' or 'ìœ¡ë¦¼ê°ì”', you **MUST** create a filter where this 'place_name' field **EXACTLY matches** the mentioned place name. Do not include other places if a specific one is named.", # ë” ê°•ë ¥í•˜ê³  ëª…í™•í•œ ì§€ì‹œ ì¶”ê°€
                type="string",
            ),
        ]

        retriever = SelfQueryRetriever.from_llm(
            llm = llm,
            vectorstore = vector_store,
            document_contents = document_contents_description,
            metadata_field_info = metadata_field_info,
            search_kwargs = {"k":20},
            verbose = True
        )

        async def generate_prompt(place_name: str, place_info: dict):

            query = place_info.get('query')
            visitorReviewScore = place_info.get('visitorReviewScore', 'N/A') # ì¥ì†Œ ë¦¬ë·° í‰ì 
            visitorReviewCount = place_info.get('visitorReviewCount', 'N/A') # ì¥ì†Œ ë¦¬ë·° ìˆ˜

            # query(ì¥ì†Œëª… í¬í•¨)ê³¼ ê´€ë ¨ëœ documents(ë¦¬ë·°ë“¤)ì„ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ì¸ docs.
            docs = await retriever.ainvoke(query)

            # ë¦¬ìŠ¤íŠ¸ docsë¥¼ í†µí•´ "-[ì¥ì†Œëª…] (ë¦¬ë·°)" í˜•íƒœì˜ contextë¥¼ ìƒì„±í•œë‹¤.
            context = "\n".join(
                f"- [ì¥ì†Œëª… : {doc.metadata.get('place_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}] {doc.page_content}"
                for doc in docs
            ).strip()

            # ë¦¬ë·° í‰ì , ë¦¬ë·° ìˆ˜ì— ëŒ€í•´ ë³„ì  = 4.52 ì (ë¦¬ë·° 268ê°œ ê¸°ë°˜)ê³¼ ê°™ì€ ë¬¸êµ¬ë¥¼ í¬í•¨í•œ ë¬¸ìì—´
            review_info = f"ë¦¬ë·° í‰ì  = {visitorReviewScore} ì (ë¦¬ë·° {visitorReviewCount} ê°œ)"

            # system_promptì˜ {context}, {question} ìë¦¬ì— ê°ê° context, queryë¥¼ ë„£ëŠ”ë‹¤.
            prompt = system_prompt.format(
                place_name = place_name,
                context = context,
                question = query
            )

            with open('output.txt', 'a', encoding='utf-8') as f:
                f.write(prompt)
                f.write("="*40)
            return prompt

        generate_prompt_tasks = [
            generate_prompt(place_name, place_info)
            for place_name, place_info in queries.items()
        ]
        prompts = await asyncio.gather(*generate_prompt_tasks)

        # print(prompts)
        results = await llm.abatch(prompts)
        # print(f"ë‹µë³€ ê²°ê³¼: {results}")

    

        # if "source_documents" in result and result["source_documents"]:
        #     for i, doc in enumerate(result["source_documents"]):
        #         place_name = doc.metadata.get("place_name", "ì¥ì†Œëª… ì—†ìŒ")

        #         review_id = doc.metadata.get("review_id", "ID ì—†ìŒ") # ë¦¬ë·° IDê°€ ìˆë‹¤ë©´ ì¶œë ¥

        #         print(f"\nğŸ“„ ë¬¸ì„œ {i+1}:")
        #         print(f"  ğŸ“ ì¥ì†Œëª…  : {place_name}")
        #         print(target_place_name == place_name)
        #         print(f"  ğŸ†” ë¦¬ë·° ID : {review_id}")
        #         print(f"  ğŸ“ ë‚´ìš© (ì¼ë¶€):")
                
        #         # page_contentë¥¼ ì ì ˆí•œ ê¸¸ì´ë¡œ ë‚˜ëˆ„ì–´ ì—¬ëŸ¬ ì¤„ë¡œ ì¶œë ¥
        #         content = doc.page_content
        #         # ë³´ê¸° ì¢‹ê²Œ 80ì ë‹¨ìœ„ë¡œ ì¤„ë°”ê¿ˆí•˜ë©°, ìµœëŒ€ 5ì¤„ (400ì) ì •ë„ë§Œ ì¶œë ¥
        #         max_lines_to_show = 5
        #         chars_per_line = 80
        #         for line_num, j in enumerate(range(0, len(content), chars_per_line)):
        #             if line_num >= max_lines_to_show:
        #                 print(f"     ...")
        #                 break
        #             print(f"     {content[j:j+chars_per_line]}")
                
        #         print("-" * 50) # ê° ë¬¸ì„œ êµ¬ë¶„ì„ 
        # else:
        #     print("  ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        # print("==================================================\n")
        # --- ì¶œë ¥ ì½”ë“œ ë ---

    


        return [result.content for result in results]
    
    except Exception as e:
        print(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") # ë””ë²„ê¹…ìš©
        print("========== ì „ì²´ íŠ¸ë ˆì´ìŠ¤ë°± ì‹œì‘ ==========")
        traceback.print_exc() # ì „ì²´ íŠ¸ë ˆì´ìŠ¤ë°±ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
        print("========== ì „ì²´ íŠ¸ë ˆì´ìŠ¤ë°± ë ==========")
        return f"ì˜¤ë¥˜: ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({e})"