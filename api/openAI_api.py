import asyncio
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.retrievers import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

import os



import langchain
import logging # Python ê¸°ë³¸ ë¡œê¹… ëª¨ë“ˆ


try:
    # Langchainì˜ ì „ì—­ verbose ëª¨ë“œ í™œì„±í™”
    langchain.globals.set_verbose(True)
    # Langchainì˜ ì „ì—­ debug ëª¨ë“œ í™œì„±í™” (ë” ìƒì„¸í•œ ë¡œê·¸)
    langchain.globals.set_debug(True)
    print("ì •ë³´: Langchain ì „ì—­ verbose ë° debug ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e_global_settings:
    print(f"ê²½ê³ : Langchain ì „ì—­ verbose/debug ì„¤ì • ì¤‘ ë¬¸ì œ ë°œìƒ: {e_global_settings}")

# Pythonì˜ ê¸°ë³¸ ë¡œê¹… ë ˆë²¨ ì„¤ì • (Langchain ë¡œê·¸ê°€ ë” ì˜ ë³´ì´ë„ë¡)
# ê¸°ë³¸ì ìœ¼ë¡œ WARNING ë ˆë²¨ ì´ìƒë§Œ ì¶œë ¥ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ INFO ë˜ëŠ” DEBUGë¡œ ë‚®ì¶¥ë‹ˆë‹¤.
try:
    logging.basicConfig(level=logging.INFO)
    # íŠ¹ì • Langchain ë¡œê±°ì˜ ë ˆë²¨ì„ ë” ë‚®ì¶œ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    logging.getLogger("langchain.retrievers.self_query").setLevel(logging.DEBUG)
    # logging.getLogger("langchain").setLevel(logging.DEBUG) # ëª¨ë“  Langchain ë¡œê·¸ë¥¼ DEBUGë¡œ
    print("ì •ë³´: Python ë¡œê¹… ë ˆë²¨ì´ INFOë¡œ ì„¤ì •ë˜ì—ˆê³ , langchain.retrievers.self_queryëŠ” DEBUGë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e_logging_config:
    print(f"ê²½ê³ : Python ë¡œê¹… ì„¤ì • ì¤‘ ë¬¸ì œ ë°œìƒ: {e_logging_config}")


system_prompt = """ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
1. ì œê³µëœ ë¬¸ë§¥ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ê¸ì •ì ì¸ ë‹µë³€ê³¼ ë¶€ì •ì ì¸ ë‹µë³€ì˜ ë¹„ìœ¨ì„ í•©í•˜ì—¬ 100%ê°€ ë˜ë„ë¡ ê°ê° ë‹µë³€í•©ë‹ˆë‹¤.
2. ë¶€ì •ì ì¸ ë‹µë³€ì—ëŠ” 'ë°°ë‹¬ì„ í•˜ì§€ ì•ŠìŒ'ê³¼ ê°™ì€ ë°°ë‹¬ ê´€ë ¨ ë¶€ì •ì  ë¦¬ë·°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
3. ë‹¤ë¥¸ ë‹µë³€ì€ í•˜ì§€ ì•Šê³ , 'ê¸ì •: n%, ë¶€ì •: n%' í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
4. ë°˜ë“œì‹œ ë¬¸ë§¥ì´ ìˆë‹¤ë©´ ê¸ì •/ë¶€ì • ë¹„ìœ¨ì„ ì¶”ì •í•´ì„œ ë‹µë³€í•˜ì„¸ìš”. 
5. queryë¡œ ì£¼ì–´ì§„ ì¥ì†Œ ì´ë¦„ê³¼ ì¼ì¹˜í•˜ëŠ” ë¦¬ë·°ë¥¼ í•˜ëŠ” ì°¸ì¡°í•´ ë‹µë³€í•©ë‹ˆë‹¤.
6. **ë¬¸ë§¥ì´ ì „í˜€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°(ë¦¬ë·° ì—†ìŒ)ì—ëŠ” 'ê¸ì •: 0%, ë¶€ì •: 0%'ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.**

ë¬¸ë§¥:
{context}

ì§ˆë¬¸:
{question}
""".strip()

async def generate_answer(queries: list, vector_store: FAISS, target_place_name: str):
    """
    ë¯¸ë¦¬ ìƒì„±ëœ Langchain FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        queries: ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        vector_store: ë¯¸ë¦¬ ìƒì„±ëœ Langchain FAISS ë²¡í„° ì €ì¥ì†Œ ê°ì²´
        target_place_name: ë¶„ì„ ëŒ€ìƒ ì¥ì†Œëª…

    Returns:
        ìƒì„±ëœ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    """
    
    try:
        # OpenAI LLM ì´ˆê¸°í™”
        # llm = ChatOpenAI(
        #     model = "gpt-4o-mini",
        #     temperature = 0,
        #     api_key = os.getenv("OPENAI_API_KEY")
        # )

        # Gemini LLM ì´ˆê¸°í™”
        llm = ChatGoogleGenerativeAI(
            model = "gemini-2.0-flash",
            temperature = 0,
            api_key = os.getenv("GEMINI_API_KEY")
        )

        document_contents_description = "A collection of user reviews for various places, primarily restaurants and cafes. Each review talks about user experiences, food, service, atmosphere, etc."

        metadata_field_info = [
            AttributeInfo(
                name="place_name",
                description="The **exact name** of the place or restaurant. If the user's query mentions a specific place name like 'ë¸Œë¦­ìŠ¤í”¼ì' or 'ìœ¡ë¦¼ê°ì”', you **MUST** create a filter where this 'place_name' field **EXACTLY matches** the mentioned place name. Do not include other places if a specific one is named.", # ë” ê°•ë ¥í•˜ê³  ëª…í™•í•œ ì§€ì‹œ ì¶”ê°€
                type="string",
            ),
        ]

        retriever = SelfQueryRetriever.from_llm(
            llm = llm,
            vectorstore = vector_store,
            document_contents = document_contents_description,
            metadata_field_info = metadata_field_info,
            search_kwargs = {"k":15, "filter": {"place_name":target_place_name}},
            verbose = True
        )

        async def generate_prompt(query):
            # query(ì¥ì†Œëª… í¬í•¨)ê³¼ ê´€ë ¨ëœ documents(ë¦¬ë·°ë“¤)ì„ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ì¸ docs.
            docs = await retriever.ainvoke(query)

            # ë¦¬ìŠ¤íŠ¸ docsë¥¼ í†µí•´ "-[ì¥ì†Œëª…] (ë¦¬ë·°)" í˜•íƒœì˜ contextë¥¼ ìƒì„±í•œë‹¤.
            context = "\n".join(
                f"- [ì¥ì†Œëª… : {doc.metadata.get('place_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}] {doc.page_content}"
                for doc in docs
            ).strip()
            print(context)

            # system_promptì˜ {context}, {question} ìë¦¬ì— ê°ê° context, queryë¥¼ ë„£ëŠ”ë‹¤.
            prompt = system_prompt.format(context = context, question = query)
            return prompt

        generate_prompt_tasks = [
            generate_prompt(query)
            for query in queries
        ]
        prompts = await asyncio.gather(*generate_prompt_tasks)

        results = await llm.abatch(prompts)
        print(f"ë‹µë³€ ê²°ê³¼: {results}")

    

        # if "source_documents" in result and result["source_documents"]:
        #     for i, doc in enumerate(result["source_documents"]):
        #         place_name = doc.metadata.get("place_name", "ì¥ì†Œëª… ì—†ìŒ")

        #         review_id = doc.metadata.get("review_id", "ID ì—†ìŒ") # ë¦¬ë·° IDê°€ ìˆë‹¤ë©´ ì¶œë ¥

        #         print(f"\nğŸ“„ ë¬¸ì„œ {i+1}:")
        #         print(f"  ğŸ“ ì¥ì†Œëª…  : {place_name}")
        #         print(target_place_name == place_name)
        #         print(f"  ğŸ†” ë¦¬ë·° ID : {review_id}")
        #         print(f"  ğŸ“ ë‚´ìš© (ì¼ë¶€):")
                
        #         # page_contentë¥¼ ì ì ˆí•œ ê¸¸ì´ë¡œ ë‚˜ëˆ„ì–´ ì—¬ëŸ¬ ì¤„ë¡œ ì¶œë ¥
        #         content = doc.page_content
        #         # ë³´ê¸° ì¢‹ê²Œ 80ì ë‹¨ìœ„ë¡œ ì¤„ë°”ê¿ˆí•˜ë©°, ìµœëŒ€ 5ì¤„ (400ì) ì •ë„ë§Œ ì¶œë ¥
        #         max_lines_to_show = 5
        #         chars_per_line = 80
        #         for line_num, j in enumerate(range(0, len(content), chars_per_line)):
        #             if line_num >= max_lines_to_show:
        #                 print(f"     ...")
        #                 break
        #             print(f"     {content[j:j+chars_per_line]}")
                
        #         print("-" * 50) # ê° ë¬¸ì„œ êµ¬ë¶„ì„ 
        # else:
        #     print("  ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        # print("==================================================\n")
        # --- ì¶œë ¥ ì½”ë“œ ë ---

    


        return [result.content for result in results]
    
    except Exception as e:
        # print(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") # ë””ë²„ê¹…ìš©
        return f"ì˜¤ë¥˜: ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({e})"